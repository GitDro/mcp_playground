#!/usr/bin/env python3
"""
Launch Script for MCP Chat Interface

Simple launcher that validates prerequisites and starts the Streamlit chat interface.

Usage:
    python launch_chat.py

Author: MCP Arena
"""

import subprocess
import sys
import time
from pathlib import Path


def check_ollama() -> bool:
    """Check if Ollama is running and has models available."""
    try:
        result = subprocess.run(
            ["ollama", "list"], 
            capture_output=True, 
            text=True, 
            timeout=10
        )
        
        if result.returncode == 0:
            lines = result.stdout.strip().split('\n')
            if len(lines) > 1:  # Header + at least one model
                print("âœ… Ollama is running with models available")
                return True
            else:
                print("âŒ Ollama is running but no models found")
                print("ğŸ’¡ Install a model with: ollama pull llama3.2")
                return False
        else:
            print("âŒ Ollama command failed")
            return False
            
    except subprocess.TimeoutExpired:
        print("âŒ Ollama command timed out")
        return False
    except FileNotFoundError:
        print("âŒ Ollama not found. Please install Ollama first.")
        return False
    except Exception as e:
        print(f"âŒ Error checking Ollama: {e}")
        return False


def check_mcp_server() -> bool:
    """Check if MCP server files exist."""
    server_file = Path("web_search_server.py")
    if server_file.exists():
        print("âœ… MCP server file found")
        return True
    else:
        print("âŒ MCP server file not found")
        return False


def launch_streamlit() -> None:
    """Launch the Streamlit chat interface."""
    try:
        print("ğŸš€ Starting Streamlit chat interface...")
        print("ğŸ“± Opening browser...")
        
        # Start Streamlit
        process = subprocess.Popen([
            sys.executable, "-m", "streamlit", "run", 
            "chat_interface.py", 
            "--server.headless", "false"
        ])
        
        # Wait a moment for Streamlit to start
        time.sleep(3)
        
        print("âœ… Chat interface is running!")
        print("ğŸŒ Open http://localhost:8501 in your browser")
        print("ğŸ›‘ Press Ctrl+C to stop")
        
        # Wait for the process
        try:
            process.wait()
        except KeyboardInterrupt:
            print("\nğŸ›‘ Shutting down...")
            process.terminate()
            process.wait()
            
    except Exception as e:
        print(f"âŒ Failed to launch Streamlit: {e}")


def main():
    """Main launcher function."""
    print("ğŸ¤– MCP Chat Interface Launcher")
    print("=" * 40)
    
    # Check prerequisites
    if not check_ollama():
        print("\nğŸ’¡ Please ensure Ollama is installed and running with models")
        sys.exit(1)
    
    if not check_mcp_server():
        print("\nğŸ’¡ Please ensure web_search_server.py is in the current directory")
        sys.exit(1)
    
    print("\nğŸ‰ All checks passed!")
    print("ğŸ“‹ Available features:")
    print("   â€¢ Multiple Ollama model selection")
    print("   â€¢ Web search integration")
    print("   â€¢ Beautiful tool call visualization")
    print("   â€¢ Real-time chat interface")
    
    print("\n" + "=" * 40)
    
    # Launch the interface
    launch_streamlit()


if __name__ == "__main__":
    main()